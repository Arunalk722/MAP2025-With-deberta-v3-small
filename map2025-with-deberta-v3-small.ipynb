{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":12587807,"sourceType":"datasetVersion","datasetId":7950186},{"sourceId":252712973,"sourceType":"kernelVersion"},{"sourceId":252715247,"sourceType":"kernelVersion"}],"dockerImageVersionId":31091,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n\n!pip install --no-index --find-links=/kaggle/input/download-compatible-offline-dependencies/offline-packages nltk sentencepiece wordcloud\n\nimport nltk\nnltk.data.path.append(\"/kaggle/input/download-compatible-offline-dependencies/nltk_data\")\n\nfrom nltk.corpus import stopwords, wordnet\n# print(\"Stopwords example:\", stopwords.words(\"english\")[:5])\n# print(\"WordNet example:\", wordnet.synsets(\"data\")[0].definition())\n\n\nprint(\"Enhanced MAP2025 Complete Solution with Deep Learning Ensemble - OFFLINE VERSION\")\nprint(\"Model Training + EDA Visualization + Validation and final Submission\")\n\n# Core libraries\nimport numpy as np\nimport pandas as pd\nimport re\nimport pickle\nimport warnings\nimport nltk\nfrom tqdm import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import f1_score, classification_report, accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\n# Set paths for offline model\nOFFLINE_MODEL_PATH = \"/kaggle/input/model-downloader/deberta-v3-small-offline\"\n\n\n# nltk.download('wordnet')\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Create directories\n\nos.makedirs('models', exist_ok=True)\nos.makedirs('eda_visualizations', exist_ok=True)\nos.makedirs('validation_results', exist_ok=True)\n\n# Configuration\ndef get_device():\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nDEVICE = get_device()\nMAX_LENGTH = 512\nBATCH_SIZE = 32\nN_FOLDS = 10\nEPOCHS = 10\n\nprint(f\"Using device: {DEVICE}\")\n\n \n# SECTION 1: DATA PREPROCESSING & FEATURE ENGINEERING\n \n\ndef extract_math_features(text):\n    if not isinstance(text, str):\n        return {\n            'frac_count': 0, 'number_count': 0, 'operator_count': 0,\n            'decimal_count': 0, 'question_mark': 0, 'math_keyword_count': 0\n        }\n    features = {\n        'frac_count': len(re.findall(r'FRAC_\\d+_\\d+|\\\\frac', text)),\n        'number_count': len(re.findall(r'\\b\\d+\\b', text)),\n        'operator_count': len(re.findall(r'[\\+\\-\\*\\/\\=]', text)),\n        'decimal_count': len(re.findall(r'\\d+\\.\\d+', text)),\n        'question_mark': int('?' in text),\n        'math_keyword_count': len(re.findall(r'solve|calculate|equation|fraction|decimal', text.lower()))\n    }\n    return features\n\ndef create_features(df):\n    # Fill missing text\n    for col in ['QuestionText', 'MC_Answer', 'StudentExplanation']:\n        df[col] = df[col].fillna('')\n    \n    # Basic length features\n    df['mc_answer_len'] = df['MC_Answer'].str.len()\n    df['explanation_len'] = df['StudentExplanation'].str.len()\n    df['question_len'] = df['QuestionText'].str.len()\n    df['explanation_to_question_ratio'] = df['explanation_len'] / (df['question_len'] + 1)\n\n    # Math-specific features\n    for col in ['QuestionText', 'MC_Answer', 'StudentExplanation']:\n        mf = df[col].apply(extract_math_features).apply(pd.Series)\n        prefix = 'mc_' if col == 'MC_Answer' else 'exp_' if col == 'StudentExplanation' else ''\n        mf.columns = [f'{prefix}{c}' for c in mf.columns]\n        df = pd.concat([df, mf], axis=1)\n\n    # Combined text for transformer\n    df['sentence'] = (\n        \"Question: \" + df['QuestionText'] +\n        \" Answer: \" + df['MC_Answer'] +\n        \" Explanation: \" + df['StudentExplanation']\n    )\n    return df\n\n \n# (EDA)\n \n\ndef perform_eda(data):\n    \"\"\"Complete EDA with visualizations\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PERFORMING EXPLORATORY DATA ANALYSIS\")\n    print(\"=\"*60)\n    \n    plt.style.use('ggplot')\n    \n    # 1. Class Distribution Analysis\n    print(\"1. Analyzing class distributions...\")\n    fig, ax = plt.subplots(1, 2, figsize=(18, 7))\n    \n    # Category distribution\n    cat_counts = data['Category'].value_counts()\n    ax[0].bar(cat_counts.index, cat_counts.values, color='skyblue')\n    ax[0].set_title('Category Distribution', fontsize=14)\n    ax[0].set_ylabel('Count', fontsize=12)\n    ax[0].tick_params(axis='x', rotation=45)\n    \n    # Misconception distribution\n    misc_counts = data['Misconception'].value_counts()\n    misc_counts = misc_counts.sort_values(ascending=False)\n    top_10_misc = misc_counts.head(10)\n    ax[1].bar(top_10_misc.index, top_10_misc.values, color='lightcoral')\n    ax[1].set_title('Top 10 Misconceptions', fontsize=14)\n    ax[1].set_ylabel('Count', fontsize=12)\n    ax[1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig('eda_visualizations/class_distribution.png', dpi=300)\n    plt.show()\n    \n    print(\"Class Distribution Summary:\")\n    print(\"Categories:\")\n    print(cat_counts)\n    print(\"\\nTop Misconceptions:\")\n    print(top_10_misc)\n    \n    # 2. Text Length Analysis\n    print(\"\\n2. Analyzing text lengths...\")\n    data['explanation_len'] = data['StudentExplanation'].str.len()\n    \n    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Distribution of explanation lengths\n    sns.histplot(data['explanation_len'], bins=50, kde=True, ax=ax[0])\n    ax[0].set_title('Distribution of Explanation Lengths', fontsize=14)\n    ax[0].set_xlabel('Character Count')\n    ax[0].set_ylabel('Frequency')\n    \n    # Length vs Category\n    top_categories = data['Category'].value_counts().index[:5]\n    sns.boxplot(\n        x='Category', \n        y='explanation_len', \n        data=data[data['Category'].isin(top_categories)],\n        ax=ax[1]\n    )\n    ax[1].set_title('Explanation Length by Top 5 Categories', fontsize=14)\n    ax[1].set_ylabel('Character Count')\n    ax[1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig('eda_visualizations/text_length_analysis.png', dpi=300)\n    plt.show()\n    \n    print(\"Text Length Statistics:\")\n    print(data['explanation_len'].describe())\n    \n    # 3. Feature Correlation Analysis\n    print(\"\\n3. Analyzing feature correlations...\")\n    # Create temp df with features\n    temp_df = create_features(data.copy())\n    \n    # Select numeric features only for correlation analysis\n    numeric_features = temp_df.select_dtypes(include=[np.number]).columns\n    feature_cols_eda = [c for c in numeric_features if c not in ['row_id', 'QuestionId']]\n    \n    if len(feature_cols_eda) > 0:\n        # Add target encodings\n        le_cat = LabelEncoder()\n        le_misc = LabelEncoder()\n        temp_df['category_encoded'] = le_cat.fit_transform(temp_df['Category'])\n        temp_df['misconception_encoded'] = le_misc.fit_transform(temp_df['Misconception'])\n        \n        # Correlation analysis\n        features_df = temp_df[feature_cols_eda + ['category_encoded', 'misconception_encoded']].fillna(0)\n        corr = features_df.corr()\n        \n        # Plot correlation heatmap for targets\n        target_corrs = corr[['category_encoded', 'misconception_encoded']].copy()\n        target_corrs = target_corrs.drop(['category_encoded', 'misconception_encoded'], axis=0)\n        \n        plt.figure(figsize=(12, 8))\n        sns.heatmap(\n            target_corrs, \n            cmap='coolwarm', \n            annot=True, \n            fmt=\".2f\",\n            square=True\n        )\n        plt.title('Feature Correlation with Targets', fontsize=16)\n        plt.tight_layout()\n        plt.savefig('eda_visualizations/feature_correlation.png', dpi=300)\n        plt.show()\n        \n        # Top correlations\n        top_cat_corrs = corr['category_encoded'].sort_values(ascending=False)[1:6]\n        top_misc_corrs = corr['misconception_encoded'].sort_values(ascending=False)[1:6]\n        \n        print(\"Top Features Correlated with Category:\")\n        print(top_cat_corrs)\n        print(\"\\nTop Features Correlated with Misconception:\")\n        print(top_misc_corrs)\n    \n    # 4. Word Cloud Analysis\n    print(\"\\n4. Creating word clouds...\")\n    base_stopwords = set(stopwords.words('english'))\n    custom_stopwords = {'would', 'could', 'one', 'two', 'three'}\n    stop_words = base_stopwords.union(custom_stopwords)\n    \n    def clean_text(text):\n        if not isinstance(text, str):\n            return \"\"\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text = re.sub(r'\\d+', '', text)\n        words = text.split()\n        return ' '.join([w for w in words if w not in stop_words and len(w) > 2])\n    \n    data['clean_explanation'] = data['StudentExplanation'].apply(clean_text)\n    \n    # Generate word clouds for top misconceptions\n    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n    axes = axes.flatten()\n    \n    top_misconceptions = data['Misconception'].value_counts().index[:6]\n    \n    for i, misc in enumerate(top_misconceptions):\n        text = \" \".join(data[data['Misconception'] == misc]['clean_explanation'])\n        if not text:\n            continue\n            \n        wordcloud = WordCloud(\n            width=800, \n            height=400,\n            background_color='white',\n            max_words=50\n        ).generate(text)\n        \n        axes[i].imshow(wordcloud, interpolation='bilinear')\n        axes[i].set_title(f\"Keywords: {misc}\", fontsize=14)\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('eda_visualizations/keyword_wordclouds.png', dpi=300)\n    plt.show()\n    \n    print(\"EDA completed! Visualizations saved to /eda_visualizations\")\n\n \n# DEEP LEARNING MODEL\n \n\n# Import transformers locally to avoid internet dependency\nfrom transformers import DebertaV2Model, DebertaV2Tokenizer\n\nclass MathMisconceptionModel(nn.Module):\n    def __init__(self, n_categories, n_misconceptions, feature_dim):\n        super().__init__()\n        # Load from offline path\n        self.bert = DebertaV2Model.from_pretrained(OFFLINE_MODEL_PATH)\n        self.tokenizer = DebertaV2Tokenizer.from_pretrained(OFFLINE_MODEL_PATH)\n        self.feature_processor = nn.Sequential(\n            nn.Linear(feature_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.category_head = nn.Sequential(\n            nn.Linear(768 + 64, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, n_categories)\n        )\n        self.misconception_head = nn.Sequential(\n            nn.Linear(768 + 64, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, n_misconceptions)\n        )\n\n    def forward(self, input_texts, features):\n        tokens = self.tokenizer(\n            input_texts,\n            padding=True,\n            truncation=True,\n            max_length=MAX_LENGTH,\n            return_tensors=\"pt\"\n        ).to(DEVICE)\n        outputs = self.bert(**tokens)\n        text_emb = outputs.last_hidden_state[:, 0, :]\n        feat_emb = self.feature_processor(features)\n        combined = torch.cat([text_emb, feat_emb], dim=1)\n        \n        return self.category_head(combined), self.misconception_head(combined)\n\n \n# DATASET & TRAINING\n \nclass MathDataset(Dataset):\n    def __init__(self, texts, features, cat_labels=None, misc_labels=None):\n        self.texts = texts\n        self.features = features\n        self.cat_labels = cat_labels\n        self.misc_labels = misc_labels\n        self.has_labels = cat_labels is not None\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {\n            'text': self.texts[idx],\n            'features': torch.tensor(self.features[idx], dtype=torch.float)\n        }\n        if self.has_labels:\n            item['cat_labels'] = torch.tensor(self.cat_labels[idx], dtype=torch.long)\n            item['misc_labels'] = torch.tensor(self.misc_labels[idx], dtype=torch.long)\n        return item\n\ndef focal_loss(logits, targets, alpha=0.75, gamma=2.0):\n    ce_loss = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n    pt = torch.exp(-ce_loss)\n    return (alpha * (1 - pt) ** gamma * ce_loss).mean()\n\ndef train_model(model, loader, optimizer):\n    model.train()\n    total_loss = 0\n    cat_preds, cat_targets = [], []\n    misc_preds, misc_targets = [], []\n    \n    for batch in tqdm(loader, desc='Training'):\n        optimizer.zero_grad()\n        cat_logits, misc_logits = model(batch['text'], batch['features'].to(DEVICE))\n        \n        loss_cat = focal_loss(cat_logits, batch['cat_labels'].to(DEVICE))\n        loss_misc = focal_loss(misc_logits, batch['misc_labels'].to(DEVICE))\n        loss = 0.6 * loss_cat + 0.4 * loss_misc\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        cat_preds.extend(torch.argmax(cat_logits, 1).cpu().numpy())\n        cat_targets.extend(batch['cat_labels'].numpy())\n        misc_preds.extend(torch.argmax(misc_logits, 1).cpu().numpy())\n        misc_targets.extend(batch['misc_labels'].numpy())\n\n    f1_cat = f1_score(cat_targets, cat_preds, average='weighted')\n    f1_misc = f1_score(misc_targets, misc_preds, average='weighted')\n    return total_loss / len(loader), f1_cat, f1_misc\n\ndef validate_model(model, loader):\n    model.eval()\n    all_cat, all_misc = [], []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc='Validating'):\n            c_logits, m_logits = model(batch['text'], batch['features'].to(DEVICE))\n            all_cat.append(torch.softmax(c_logits, 1).cpu().numpy())\n            all_misc.append(torch.softmax(m_logits, 1).cpu().numpy())\n    return np.vstack(all_cat), np.vstack(all_misc)\n\n \n#VALIDATION & ACCURACY CALCULATION\n \n\ndef evaluate_fold_predictions(true_categories, pred_categories, true_misconceptions, pred_misconceptions, fold_num):\n    \"\"\"Calculate detailed accuracy metrics for a fold\"\"\"\n    \n    # Category metrics\n    cat_accuracy = accuracy_score(true_categories, pred_categories)\n    cat_f1 = f1_score(true_categories, pred_categories, average='weighted')\n    \n    # Misconception metrics\n    misc_accuracy = accuracy_score(true_misconceptions, pred_misconceptions)\n    misc_f1 = f1_score(true_misconceptions, pred_misconceptions, average='weighted')\n    \n    # Combined metric (what the competition uses)\n    combined_predictions = [f\"{cat}:{misc}\" for cat, misc in zip(pred_categories, pred_misconceptions)]\n    combined_true = [f\"{cat}:{misc}\" for cat, misc in zip(true_categories, true_misconceptions)]\n    combined_accuracy = accuracy_score(combined_true, combined_predictions)\n    \n    results = {\n        'fold': fold_num,\n        'category_accuracy': cat_accuracy,\n        'category_f1': cat_f1,\n        'misconception_accuracy': misc_accuracy,\n        'misconception_f1': misc_f1,\n        'combined_accuracy': combined_accuracy\n    }\n    \n    print(f\"Fold {fold_num} Results:\")\n    print(f\"  Category Accuracy: {cat_accuracy:.4f}\")\n    print(f\"  Category F1: {cat_f1:.4f}\")\n    print(f\"  Misconception Accuracy: {misc_accuracy:.4f}\")\n    print(f\"  Misconception F1: {misc_f1:.4f}\")\n    print(f\"  Combined Accuracy: {combined_accuracy:.4f}\")\n    \n    return results\n\ndef save_detailed_predictions(df, fold_predictions, cat_enc, misc_enc):\n    \"\"\"Save detailed predictions with probabilities and comparisons\"\"\"\n    \n    detailed_results = []\n    \n    for fold_num, (indices, cat_probs, misc_probs) in fold_predictions.items():\n        fold_df = df.iloc[indices].copy()\n        \n        # Get predictions\n        cat_preds = np.argmax(cat_probs, axis=1)\n        misc_preds = np.argmax(misc_probs, axis=1)\n        \n        # Convert to labels\n        pred_categories = cat_enc.inverse_transform(cat_preds)\n        pred_misconceptions = misc_enc.inverse_transform(misc_preds)\n        \n        # Create detailed dataframe\n        for i, idx in enumerate(indices):\n            result = {\n                'fold': fold_num,\n                'row_id': df.iloc[idx]['row_id'],\n                'QuestionText': df.iloc[idx]['QuestionText'][:100] + \"...\",  # Truncate for readability\n                'MC_Answer': df.iloc[idx]['MC_Answer'],\n                'StudentExplanation': df.iloc[idx]['StudentExplanation'][:150] + \"...\",  # Truncate\n                'true_category': df.iloc[idx]['Category'],\n                'predicted_category': pred_categories[i],\n                'category_correct': df.iloc[idx]['Category'] == pred_categories[i],\n                'true_misconception': df.iloc[idx]['Misconception'],\n                'predicted_misconception': pred_misconceptions[i],\n                'misconception_correct': df.iloc[idx]['Misconception'] == pred_misconceptions[i],\n                'combined_correct': (df.iloc[idx]['Category'] == pred_categories[i]) and \n                                   (df.iloc[idx]['Misconception'] == pred_misconceptions[i])\n            }\n            \n            # Add top 3 category probabilities\n            top_cat_indices = np.argsort(cat_probs[i])[-3:][::-1]\n            for j, cat_idx in enumerate(top_cat_indices):\n                result[f'top_{j+1}_category'] = cat_enc.inverse_transform([cat_idx])[0]\n                result[f'top_{j+1}_category_prob'] = cat_probs[i][cat_idx]\n            \n            # Add top 3 misconception probabilities\n            top_misc_indices = np.argsort(misc_probs[i])[-3:][::-1]\n            for j, misc_idx in enumerate(top_misc_indices):\n                result[f'top_{j+1}_misconception'] = misc_enc.inverse_transform([misc_idx])[0]\n                result[f'top_{j+1}_misconception_prob'] = misc_probs[i][misc_idx]\n            \n            detailed_results.append(result)\n    \n    # Save to CSV\n    detailed_df = pd.DataFrame(detailed_results)\n    detailed_df.to_csv('validation_results/detailed_predictions.csv', index=False)\n    \n    # Create summary by fold\n    summary_results = []\n    for fold_num in detailed_df['fold'].unique():\n        fold_data = detailed_df[detailed_df['fold'] == fold_num]\n        summary = {\n            'fold': fold_num,\n            'total_samples': len(fold_data),\n            'category_accuracy': fold_data['category_correct'].mean(),\n            'misconception_accuracy': fold_data['misconception_correct'].mean(),\n            'combined_accuracy': fold_data['combined_correct'].mean()\n        }\n        summary_results.append(summary)\n    \n    summary_df = pd.DataFrame(summary_results)\n    summary_df.to_csv('validation_results/fold_summary.csv', index=False)\n    \n    print(\"Detailed predictions saved to validation_results/detailed_predictions.csv\")\n    print(\"Fold summary saved to validation_results/fold_summary.csv\")\n    \n    return detailed_df, summary_df\n\n \n#MAIN WORKFLOW\n \n\ndef main():\n    print(\"Starting Complete MAP2025 Solution Pipeline - OFFLINE VERSION\")\n    \n    # Load datasets\n    print(\"\\nLoading datasets...\")\n    train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n    test = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n    \n    print(f\"Train shape: {train.shape}\")\n    print(f\"Test shape: {test.shape}\")\n    \n    # Fill missing values\n    train['Misconception'] = train['Misconception'].fillna('NA')\n    \n    # PERFORM EDA\n    perform_eda(train)\n    \n    # Feature engineering\n    print(\"\\nCreating features...\")\n    train = create_features(train)\n    test = create_features(test)\n    \n    # Encode targets\n    cat_enc = LabelEncoder().fit(train['Category'])\n    misc_enc = LabelEncoder().fit(train['Misconception'])\n    train['category_encoded'] = cat_enc.transform(train['Category'])\n    train['misconception_encoded'] = misc_enc.transform(train['Misconception'])\n    \n    # Save encoders\n    with open('models/cat_encoder.pkl', 'wb') as f:\n        pickle.dump(cat_enc, f)\n    with open('models/misc_encoder.pkl', 'wb') as f:\n        pickle.dump(misc_enc, f)\n    print(\"Label encoders saved.\")\n    \n    # Select features - only numeric columns\n    drop_cols = [\n        'Category', 'Misconception', 'sentence',\n        'QuestionText', 'MC_Answer', 'StudentExplanation',\n        'category_encoded', 'misconception_encoded',\n        'clean_explanation'\n    ]\n    \n    # Get only numeric columns and exclude unwanted ones\n    numeric_cols = train.select_dtypes(include=[np.number]).columns\n    feature_cols = [c for c in numeric_cols if c not in drop_cols and not c.startswith('Unnamed')]\n    \n    print(f\"Selected {len(feature_cols)} features: {feature_cols[:10]}...\")\n    \n    # Save feature columns\n    with open('models/feature_cols.pkl', 'wb') as f:\n        pickle.dump(feature_cols, f)\n    print(\"Feature columns saved.\")\n    \n    # Scale features\n    for col in feature_cols:\n        if col not in test.columns:\n            test[col] = 0\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(train[feature_cols].fillna(0))\n    X_test = scaler.transform(test[feature_cols].fillna(0))\n    \n    # Prepare data\n    texts_train = train['sentence'].tolist()\n    texts_test = test['sentence'].tolist()\n    y_cat = train['category_encoded'].values\n    y_misc = train['misconception_encoded'].values\n    \n    # Cross-validation setup\n    kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n    oof_cat_preds = np.zeros((len(train), len(cat_enc.classes_)))\n    oof_misc_preds = np.zeros((len(train), len(misc_enc.classes_)))\n    \n    # For validation tracking\n    fold_predictions = {}\n    fold_metrics = []\n    \n    # Test predictions\n    test_cat_trad = np.zeros((len(test), len(cat_enc.classes_)))\n    test_cat_dl = np.zeros((len(test), len(cat_enc.classes_)))\n    test_misc_trad = np.zeros((len(test), len(misc_enc.classes_)))\n    test_misc_dl = np.zeros((len(test), len(misc_enc.classes_)))\n    \n    # Cross-validation training\n    print(f\"\\nStarting {N_FOLDS}-Fold Cross-Validation Training...\")\n    \n    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train, y_cat)):\n        print(f\"\\n====== Fold {fold+1}/{N_FOLDS} ======\")\n        X_tr, X_va = X_train[tr_idx], X_train[va_idx]\n        ycat_tr, ycat_va = y_cat[tr_idx], y_cat[va_idx]\n        ymis_tr, ymis_va = y_misc[tr_idx], y_misc[va_idx]\n        txt_tr = [texts_train[i] for i in tr_idx]\n        txt_va = [texts_train[i] for i in va_idx]\n        \n        # Traditional models\n        print(\"Training traditional models...\")\n        cat_model = LogisticRegression(class_weight='balanced', max_iter=1000, solver='lbfgs')\n        misc_model = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n        cat_model.fit(X_tr, ycat_tr)\n        misc_model.fit(X_tr, ymis_tr)\n        \n        # Traditional predictions\n        test_cat_trad += cat_model.predict_proba(X_test) / N_FOLDS\n        test_misc_trad += misc_model.predict_proba(X_test) / N_FOLDS\n        \n        # Deep Learning\n        print(\"Training deep learning model...\")\n        train_ds = MathDataset(txt_tr, X_tr, ycat_tr, ymis_tr)\n        val_ds = MathDataset(txt_va, X_va, ycat_va, ymis_va)\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n        test_ds = MathDataset(texts_test, X_test)\n        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n        \n        model = MathMisconceptionModel(\n            n_categories=len(cat_enc.classes_),\n            n_misconceptions=len(misc_enc.classes_),\n            feature_dim=X_train.shape[1]\n        ).to(DEVICE)\n        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)\n        \n        best_score = 0\n        for epoch in range(1, EPOCHS+1):\n            tr_loss, tr_f1_cat, tr_f1_misc = train_model(model, train_loader, optimizer)\n            val_cat_probs, _ = validate_model(model, val_loader)\n            val_preds = np.argmax(val_cat_probs, axis=1)\n            val_score = f1_score(ycat_va, val_preds, average='weighted')\n            \n            print(f\"Epoch {epoch}/{EPOCHS} | Loss: {tr_loss:.4f} | Cat F1: {tr_f1_cat:.4f} | Val Cat F1: {val_score:.4f}\")\n            \n            if val_score > best_score:\n                best_score = val_score\n                torch.save(model.state_dict(), f'models/map_2025_best_model_fold{fold}.pt')\n        \n        # Load best model and get predictions\n        model.load_state_dict(torch.load(f'models/map_2025_best_model_fold{fold}.pt'))\n        \n        # OOF validation predictions\n        val_cat_probs, val_misc_probs = validate_model(model, val_loader)\n        oof_cat_preds[va_idx] = val_cat_probs\n        oof_misc_preds[va_idx] = val_misc_probs\n        \n        # Store fold predictions for validation\n        fold_predictions[fold] = (va_idx, val_cat_probs, val_misc_probs)\n        \n        # Calculate fold metrics\n        true_cats = train.iloc[va_idx]['Category'].values\n        true_miscs = train.iloc[va_idx]['Misconception'].values\n        pred_cats = cat_enc.inverse_transform(np.argmax(val_cat_probs, axis=1))\n        pred_miscs = misc_enc.inverse_transform(np.argmax(val_misc_probs, axis=1))\n        \n        fold_result = evaluate_fold_predictions(true_cats, pred_cats, true_miscs, pred_miscs, fold)\n        fold_metrics.append(fold_result)\n        \n        # Test predictions\n        te_cat_probs, te_misc_probs = validate_model(model, test_loader)\n        test_cat_dl += te_cat_probs / N_FOLDS\n        test_misc_dl += te_misc_probs / N_FOLDS\n    \n     \n    # VALIDATION ANALYSIS & DETAILED REPORTING\n     \n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"VALIDATION ANALYSIS & DETAILED REPORTING\")\n    print(\"=\"*60)\n    \n    # Save detailed predictions\n    detailed_df, summary_df = save_detailed_predictions(train, fold_predictions, cat_enc, misc_enc)\n    \n    # Overall validation metrics\n    print(\"\\nOverall Cross-Validation Results:\")\n    metrics_df = pd.DataFrame(fold_metrics)\n    \n    overall_metrics = {\n        'Mean Category Accuracy': metrics_df['category_accuracy'].mean(),\n        'Std Category Accuracy': metrics_df['category_accuracy'].std(),\n        'Mean Category F1': metrics_df['category_f1'].mean(),\n        'Std Category F1': metrics_df['category_f1'].std(),\n        'Mean Misconception Accuracy': metrics_df['misconception_accuracy'].mean(),\n        'Std Misconception Accuracy': metrics_df['misconception_accuracy'].std(),\n        'Mean Misconception F1': metrics_df['misconception_f1'].mean(),\n        'Std Misconception F1': metrics_df['misconception_f1'].std(),\n        'Mean Combined Accuracy': metrics_df['combined_accuracy'].mean(),\n        'Std Combined Accuracy': metrics_df['combined_accuracy'].std()\n    }\n    \n    for metric, value in overall_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Save metrics\n    metrics_df.to_csv('validation_results/cv_metrics.csv', index=False)\n    \n    # Create confusion matrices for best fold\n    best_fold_idx = metrics_df['combined_accuracy'].idxmax()\n    best_fold_num = metrics_df.iloc[best_fold_idx]['fold']\n    best_fold_data = detailed_df[detailed_df['fold'] == best_fold_num]\n    \n    # Category confusion matrix\n    plt.figure(figsize=(12, 8))\n    cm_cat = confusion_matrix(best_fold_data['true_category'], best_fold_data['predicted_category'])\n    sns.heatmap(cm_cat, annot=True, fmt='d', cmap='Blues', \n                xticklabels=cat_enc.classes_, yticklabels=cat_enc.classes_)\n    plt.title(f'Category Confusion Matrix - Best Fold ({best_fold_num})')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig('validation_results/category_confusion_matrix.png', dpi=300)\n    plt.show()\n    \n    # Top misconceptions confusion matrix (limit to top 10)\n    top_misconceptions = train['Misconception'].value_counts().head(10).index\n    best_fold_top_misc = best_fold_data[\n        (best_fold_data['true_misconception'].isin(top_misconceptions)) |\n        (best_fold_data['predicted_misconception'].isin(top_misconceptions))\n    ]\n    \n    if len(best_fold_top_misc) > 0:\n        plt.figure(figsize=(15, 12))\n        cm_misc = confusion_matrix(best_fold_top_misc['true_misconception'], \n                                   best_fold_top_misc['predicted_misconception'])\n        \n        # Get unique labels from both true and predicted\n        unique_labels = sorted(list(set(best_fold_top_misc['true_misconception'].unique()) | \n                                   set(best_fold_top_misc['predicted_misconception'].unique())))\n        \n        sns.heatmap(cm_misc, annot=True, fmt='d', cmap='Reds',\n                    xticklabels=unique_labels[:len(cm_misc[0])], \n                    yticklabels=unique_labels[:len(cm_misc)])\n        plt.title(f'Top Misconceptions Confusion Matrix - Best Fold ({best_fold_num})')\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=0)\n        plt.tight_layout()\n        plt.savefig('validation_results/misconception_confusion_matrix.png', dpi=300)\n        plt.show()\n    \n    # Error analysis \n    print(\"\\nError Analysis:\")\n    \n    # Category errors\n    cat_errors = detailed_df[detailed_df['category_correct'] == False]\n    if len(cat_errors) > 0:\n        print(\"\\nMost Common Category Prediction Errors:\")\n        error_summary = cat_errors.groupby(['true_category', 'predicted_category']).size().sort_values(ascending=False).head(10)\n        for (true_cat, pred_cat), count in error_summary.items():\n            print(f\"  {true_cat} → {pred_cat}: {count} times\")\n    \n    # Misconception errors\n    misc_errors = detailed_df[detailed_df['misconception_correct'] == False]\n    if len(misc_errors) > 0:\n        print(\"\\nMost Common Misconception Prediction Errors:\")\n        error_summary = misc_errors.groupby(['true_misconception', 'predicted_misconception']).size().sort_values(ascending=False).head(10)\n        for (true_misc, pred_misc), count in error_summary.items():\n            print(f\"  {true_misc} → {pred_misc}: {count} times\")\n    \n    # Performance by category\n    print(\"\\nPerformance by Category:\")\n    category_performance = detailed_df.groupby('true_category').agg({\n        'category_correct': 'mean',\n        'misconception_correct': 'mean',\n        'combined_correct': 'mean'\n    }).round(4)\n    category_performance.columns = ['Category_Accuracy', 'Misconception_Accuracy', 'Combined_Accuracy']\n    print(category_performance)\n    \n    category_performance.to_csv('validation_results/category_performance.csv')\n        \n    \n     \n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"CREATING FINAL ENSEMBLE & SUBMISSION\")\n    print(\"=\"*60)\n    \n    # Ensemble predictions for test set\n    test_cat_probs = 0.7 * test_cat_dl + 0.3 * test_cat_trad\n    test_misc_probs = 0.7 * test_misc_dl + 0.3 * test_misc_trad\n    \n    test_cat = np.argmax(test_cat_probs, axis=1)\n    test_misc = np.argmax(test_misc_probs, axis=1)\n    \n    # Convert to original labels\n    test['Category'] = cat_enc.inverse_transform(test_cat)\n    test['Misconception'] = misc_enc.inverse_transform(test_misc)\n    test['Category:Misconception'] = test['Category'] + ':' + test['Misconception']\n    \n    # Create submission\n    submission = test[['row_id', 'Category:Misconception']]\n    submission.to_csv('submission.csv', index=False)\n    print('Submission saved to submission.csv')\n    \n    # Show sample predictions\n    print(\"\\nSample Test Predictions:\")\n    sample_preds = test[['row_id', 'QuestionText', 'MC_Answer', 'StudentExplanation', \n                        'Category', 'Misconception', 'Category:Misconception']].head(10)\n    \n    for idx, row in sample_preds.iterrows():\n        print(f\"\\nRow {row['row_id']}:\")\n        print(f\"  Question: {row['QuestionText'][:100]}...\")\n        print(f\"  Answer: {row['MC_Answer']}\")\n        print(f\"  Explanation: {row['StudentExplanation'][:100]}...\")\n        print(f\"  Predicted: {row['Category:Misconception']}\")\n    \n     \n    # FINAL VALIDATION WITH TRAIN.CSV     \n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL VALIDATION WITH TRAIN.CSV\")\n    print(\"=\"*60)\n    \n    # Load original training data for comparison\n    original_train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n    original_train['Misconception'] = original_train['Misconception'].fillna('NA')\n    \n    # Get OOF predictions for entire training set\n    oof_cat_preds_labels = cat_enc.inverse_transform(np.argmax(oof_cat_preds, axis=1))\n    oof_misc_preds_labels = misc_enc.inverse_transform(np.argmax(oof_misc_preds, axis=1))\n    oof_combined = [f\"{cat}:{misc}\" for cat, misc in zip(oof_cat_preds_labels, oof_misc_preds_labels)]\n    \n    # True labels\n    true_combined = [f\"{cat}:{misc}\" for cat, misc in zip(original_train['Category'], original_train['Misconception'])]\n    \n    # Final validation metrics\n    final_cat_accuracy = accuracy_score(original_train['Category'], oof_cat_preds_labels)\n    final_misc_accuracy = accuracy_score(original_train['Misconception'], oof_misc_preds_labels)\n    final_combined_accuracy = accuracy_score(true_combined, oof_combined)\n    \n    final_cat_f1 = f1_score(original_train['Category'], oof_cat_preds_labels, average='weighted')\n    final_misc_f1 = f1_score(original_train['Misconception'], oof_misc_preds_labels, average='weighted')\n    final_combined_f1 = f1_score(true_combined, oof_combined, average='weighted')\n    \n    print(\"FINAL OUT-OF-FOLD VALIDATION RESULTS:\")\n    print(f\"Category Accuracy: {final_cat_accuracy:.4f}\")\n    print(f\"Category F1 Score: {final_cat_f1:.4f}\")\n    print(f\"Misconception Accuracy: {final_misc_accuracy:.4f}\")\n    print(f\"Misconception F1 Score: {final_misc_f1:.4f}\")\n    print(f\"Combined Accuracy: {final_combined_accuracy:.4f}\")\n    print(f\"Combined F1 Score: {final_combined_f1:.4f}\")\n    \n    # Create final comparison dataframe\n    final_comparison = pd.DataFrame({\n        'row_id': original_train['row_id'],\n        'QuestionText': original_train['QuestionText'].str[:100] + \"...\",\n        'MC_Answer': original_train['MC_Answer'],\n        'StudentExplanation': original_train['StudentExplanation'].str[:150] + \"...\",\n        'true_category': original_train['Category'],\n        'predicted_category': oof_cat_preds_labels,\n        'category_correct': original_train['Category'] == oof_cat_preds_labels,\n        'true_misconception': original_train['Misconception'],\n        'predicted_misconception': oof_misc_preds_labels,\n        'misconception_correct': original_train['Misconception'] == oof_misc_preds_labels,\n        'true_combined': true_combined,\n        'predicted_combined': oof_combined,\n        'combined_correct': np.array(true_combined) == np.array(oof_combined)\n    })\n    \n    # Add prediction probabilities\n    for i, class_name in enumerate(cat_enc.classes_):\n        final_comparison[f'prob_cat_{class_name}'] = oof_cat_preds[:, i]\n    \n    for i, class_name in enumerate(misc_enc.classes_):\n        final_comparison[f'prob_misc_{class_name}'] = oof_misc_preds[:, i]\n    \n    final_comparison.to_csv('validation_results/final_train_comparison.csv', index=False)\n    print(\"Final train comparison saved to validation_results/final_train_comparison.csv\")\n    \n    # Classification reports\n    print(\"\\nDETAILED CLASSIFICATION REPORTS:\")\n    \n    print(\"\\nCategory Classification Report:\")\n    cat_report = classification_report(original_train['Category'], oof_cat_preds_labels)\n    print(cat_report)\n    \n    print(\"\\nMisconception Classification Report (Top 10):\")\n    top_misconceptions = original_train['Misconception'].value_counts().head(10).index\n    mask = original_train['Misconception'].isin(top_misconceptions)\n    misc_report = classification_report(\n        original_train[mask]['Misconception'], \n        np.array(oof_misc_preds_labels)[mask]\n    )\n    print(misc_report)\n    \n    # Save classification reports\n    with open('validation_results/classification_reports.txt', 'w') as f:\n        f.write(\"CATEGORY CLASSIFICATION REPORT\\n\")\n        f.write(\"=\"*50 + \"\\n\")\n        f.write(cat_report)\n        f.write(\"\\n\\nMISCONCEPTION CLASSIFICATION REPORT (TOP 10)\\n\")\n        f.write(\"=\"*50 + \"\\n\")\n        f.write(misc_report)\n         \n    # FINAL SUMMARY     \n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" COMPLETE MAP2025 SOLUTION PIPELINE FINISHED!\")\n    print(\"=\"*80)    \n        \n    print(f\"\\n Final Model Performance:\")\n    print(f\"    • Combined Accuracy: {final_combined_accuracy:.4f}\")\n    print(f\"    • Combined F1 Score: {final_combined_f1:.4f}\")\n    print(f\"    • Category F1 Score: {final_cat_f1:.4f}\")\n    print(f\"    • Misconception F1 Score: {final_misc_f1:.4f}\")\n    \n    print(\"\\n All tasks completed successfully!\")\n    \n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T05:32:42.099033Z","iopub.execute_input":"2025-07-27T05:32:42.099325Z","iopub.status.idle":"2025-07-27T05:34:13.798914Z","shell.execute_reply.started":"2025-07-27T05:32:42.099274Z","shell.execute_reply":"2025-07-27T05:34:13.797936Z"}},"outputs":[],"execution_count":null}]}